\documentclass{beamer}
\usetheme{metropolis}
\usecolortheme{spruce}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{listings}
\lstset{
	basicstyle=\ttfamily\tiny, % Adjusted for richer content
	breaklines=true,
	frame=single,
	numbers=left,
	numberstyle=\tiny\color{gray},
	backgroundcolor=\color{gray!10},
	keywordstyle=\color{blue},
	stringstyle=\color{red},
	commentstyle=\color{green!60!black},
	language=R % Default, switches to Python
}

% Title slide information
\title{Regression \& Prediction}
\subtitle{Theory and Practice with House Prices}
\author{Your Name}
\institute{xAI}
\date{February 23, 2025}

\begin{document}
	
	% Title Slide
	\begin{frame}
		\titlepage
	\end{frame}
	
	% Outline Slide
	\begin{frame}{Our Housing Journey}
		\begin{columns}
			\column{0.5\textwidth}
			\begin{itemize}
				\item Starting Simple: Foundations
				\item Expanding the Scope: Complexity
				\item Refining Precision: Optimization
			\end{itemize}
			\column{0.5\textwidth}
			\begin{itemize}
				\item Facing Challenges: Pitfalls
				\item Insights \& Next Steps: Applications
			\end{itemize}
		\end{columns}
		\metroset{block=fill}
		\begin{block}{Focus}
			Unpacking King County house prices with rich theory and dual R/Python implementations
		\end{block}
	\end{frame}
	
	% Outline Slide
	\begin{frame}{The Quest to Understand Relationships}
		\begin{itemize}
			\item Imagine you are exploring data to understand how different factors relate to each other.
			\item Thatâ€™s regression: a tool to ask, ``How does $Y$ change with $X$, and can we predict it?''
			\item In very basically Itâ€™s the bridge between stats, where we explain the past, and data science, where we predict the futureâ€”our journey begins here.
		\end{itemize}
	\end{frame}
	
	
	% Section 1: Starting Simple
	\section{Starting Simple}
	
	\begin{frame}{The Housing Puzzle}
		\begin{itemize}
			\item \textbf{Objective}: Decode drivers of house prices in King Countyâ€”size, location, features
			\item \textbf{Regression}: A statistical lens linking price ($Y$) to predictors like size ($X$)
			\item \textbf{Purpose}: Explain historical sales patterns and forecast future values for buyers and assessors
		\end{itemize}
	\end{frame}
	
	\begin{frame}[fragile]{Simple Linear Regression: Theory \& R}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Theory}: Models a straight-line relationship: $Y = b_0 + b_1X + e$
				\item $b_0$: Base price when size is zero; $b_1$: Price increase per sq ft; $e$: Random error
				\item Assumes linearity and independenceâ€”foundation of regression
			\end{itemize}
			\begin{lstlisting}
# R 
simple_lm <- lm(AdjSalePrice ~ SqFtTotLiving, data = house_df)
# Create the plot
ggplot(house_df, aes(x = sqft_living, y = price)) +
geom_point(alpha = 0.5) +  # Scatter plot
geom_smooth(method = "lm", color = "blue", se = FALSE) +  # Regression line
labs(title = "Price vs. Size Fit",
x = "Size (sqft_living)",
y = "Price") +
theme_minimal()
# Output: Coefficients:
(Intercept)  sqft_living  
-43580.7        280.6 

			\end{lstlisting}
			\column{0.4\textwidth}
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{regres1.jpg} % Use .png, .jpg, or .jpeg instead of .pdf
			\caption{Price vs. Size Fit}
			\label{fig:price_vs_size_fit}
			\begin{itemize}
				\item Size as a core driver
			\end{itemize}
			\begin{itemize}
				\item b0=âˆ’43580.7 , b1=280.6
			\end{itemize}
		\end{figure}
		\end{columns}
	\end{frame}
	

	
	\begin{frame}{Finding the Best Fit: Least Squares}
			\begin{itemize}
				 \item \textbf{How do we draw that line?} We minimize the messâ€”sum of squared errors
				\item \textbf{Theory}: Finds the line minimizing residual sum of squares: $\sum_{i=1}^n (Y_i - \hat{Y}_i)^2$
				\item \textbf{How}: Adjusts $b_0$ and $b_1$ to reduce prediction errorsâ€”optimal for linear fits
				\item \textbf{History}: Legendre (1805) and Gauss; computationally efficient but outlier-sensitive in small datasets
			\end{itemize}
		\end{frame}
		
	


	
	
	% Section 2: Expanding the Scope
	\section{Expanding the Scope}
	
	\begin{frame}[fragile]{More Clues: Multiple Linear Regression}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Theory}: Extends to multiple predictors: $Y = b_0 + b_1X_1 + b_2X_2 + \dots + e$
				\item \textbf{Power}: Captures combined effectsâ€”size, lot, bedroomsâ€”assuming linearity
				\item \textbf{Use}: Explains complex housing dynamics
			\end{itemize}
			\begin{lstlisting}
# R 
house_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +
Bedrooms + BldgGrade, data = house)
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{itemize}
				\item Size adds \$229/sq ft
			\end{itemize}
		\end{columns}
	\end{frame}
	

% Section 2: Key Findings

\begin{frame}[fragile]{Multiple Linear Regression: Key Findings}
	\begin{columns}
		\column{0.9\textwidth}
		\begin{lstlisting}
house_lm <- lm(price ~ sqft_living + sqft_lot + bathrooms + grade, 
data = house_df, na.action = na.omit)
Coefficients:
(Intercept)  sqft_living     sqft_lot    bathrooms        grade  
-5.957e+05    2.313e+02   -3.254e-01   -2.797e+04    9.559e+04  
		\end{lstlisting}
		\begin{itemize}
			\item \textbf{sqft\_living}: +\ðŸ¡ 2.313e+02 per sq ft â†’ Larger houses increase price significantly.
			\item \textbf{grade}: ðŸ”¼ +\$9.559e+04 per unit â†’ Higher quality homes boost price.
			\item \textbf{sqft\_lot}: ðŸŒ² -\$3.254e-01 per sq ft â†’ Lot size has a tiny negative effect.
			\item \textbf{bathrooms}: ðŸš¿ -\$4.074e+04 per extra bathroom â†’ Unexpected negative impact.
		\end{itemize}
	\end{columns}
\end{frame}

% Section 3: Model Performance
\begin{frame}[fragile]{Multiple Linear Regression:Model Evaluation}
	\begin{columns}
		\column{0.9\textwidth}
		\begin{lstlisting}
summary(house_lm)# Model Summary (Extracts Coefficients, p-values, RÂ²)
predictions <- predict(house_lm, newdata = house_df)# Extract RMSE
residuals <- house_df$price - predictions
RMSE <- sqrt(mean(residuals^2))
R_squared <- summary(house_lm)$r.squared # Extract R-squared
p_values <- summary(house_lm)$coefficients[, 4] # Extract P-values
# Print Results
> cat("RMSE:", RMSE, "\n") | RMSE: 249532.2 
> cat("RÂ²:", R_squared, "\n") | RÂ²: 0.5380018 
> cat("P-values:\n") > print(p_values)
print(p_values)
		\end{lstlisting}
		\begin{lstlisting}
(Intercept)  sqft_living     sqft_lot    bathrooms        grade 
0.000000e+00 0.000000e+00 1.711092e-10 2.689854e-30 0.000000e+00 
		\end{lstlisting}
		\begin{itemize}
			\item \textbf{RMSE}:  \$249,532 Predictions are off by 261K on avg.
			\item \textbf{$R^2$}: ðŸ“Š 0.5406 (54.06\%) â†’ The model explains 54\% of price variation..
			\item \textbf{P-values}: ðŸ§ª SqFtLot is not statistically significant `(A small p-value (< 0.05) means the predictor is statistically significant, while a large p-value (> 0.05) means it is not useful for prediction.)
		\end{itemize}
	\end{columns}
\end{frame}

	
	% Section 2: Expanding the Scope
	\begin{frame}{Encoding Categorical Variables in Regression Models}
		\begin{itemize}
			\item \textbf{Categorical variables} must be converted into numerical values for regression.
			\item \textbf{Encoding methods:}
			\begin{itemize}
				\item \textbf{Dummy (One-Hot) Encoding} â€“ Creates binary columns for each category.
				\item \textbf{Reference (Treatment) Coding} â€“ Uses one category as a reference, keeping $P - 1$ columns.
				\item \textbf{Deviation (Sum) Coding} â€“ Compares each category to the overall mean.
				\item \textbf{Ordered Factor Encoding} â€“ Converts ordered categories into numeric values.
			\end{itemize}
			\item \textbf{How itâ€™s used in regression:}
			\begin{itemize}
				\item Each encoded category appears as a separate coefficient.
				\item The model estimates how each category affects the outcome relative to the reference level.
				\item For ordered factors, treating them as numeric assumes a linear relationship.
			\end{itemize}
		\end{itemize}
	\end{frame}
	

	\begin{frame}[fragile]{Multiple Linear Regression: Categorical Variables}
		\begin{columns}
			\column{0.9\textwidth}
			\begin{lstlisting}
# Ensure PropertyType has a valid reference level
house$PropertyType <- relevel(house$PropertyType, ref = "Multiplex")

# Fit the regression model with the refined dataset
house_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + BldgGrade + PropertyType, data = house)
Coefficients:
Estimate Std. Error t value Pr(>|t|)   
(Intercept)               173429.87   31512.77   5.503  0.00271 **
...
BldgGrade                  -1442.60    6734.35  -0.214  0.83884   
PropertyTypeSingle Family   1456.54    8063.28   0.181  0.86374   
PropertyTypeTownhouse       9677.07   11444.71   0.846  0.43639 
			\end{lstlisting}
			\begin{itemize}
				\item Sets "Multiplex" as the reference category, ensuring comparisons against it.
				\item "Single Family" and "Townhouse" to have separate coefficients in the regression output.
			\end{itemize}
		\end{columns}
	\end{frame}

	
	
	\begin{frame}[fragile]{Factor Variables: Python}
		\lstset{language=Python}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Practice}: Integrates property type into price model
				\item \textbf{Key Insight}: Townhouses may differ from single-family homes
			\end{itemize}
			\begin{lstlisting}
# Python (p. 166 adapted)
import pandas as pd
X = pd.get_dummies(house['PropertyType'], drop_first=True)
# Drops first level (e.g., Multiplex) as reference
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{itemize}
				\item Baseline comparison
			\end{itemize}
		\end{columns}
	\end{frame}
	
	\begin{frame}[fragile]{Nonlinear Fit}
		\begin{columns}
			\column{0.5\textwidth}
			\begin{itemize}
				\item Nonlinear via polynomial segments joined at knots
				\item \textbf{Why}: Captures diminishing returnsâ€”e.g., small homes gain more per sq ft
				\item \textbf{Advantage}: Flexible fit.
			\end{itemize}
			\begin{lstlisting}
# Load data (replace 'house_98105' with your dataset)
model_poly <- lm(price ~ poly(sqft_living, 2) + sqft_lot +
grade + bathrooms + bedrooms, data=house_df)

# Generate partial residual plot
visreg(model_poly, "sqft_living", gg=TRUE) +
geom_point(color = "black", shape = 1) + 
geom_smooth(method="loess", color="blue", size=1.2, linetype="dashed") + 
theme_minimal() +
labs(title="Partial Residual Plot", x="SqFtTotLiving", y="Partial Residuals")
			\end{lstlisting}
			\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=\linewidth]{without_poly.png}
				\caption{Without poly.}
			\end{figure}
		\end{columns}
	\end{frame}
	
	
\begin{frame}{Comparison of Result}
	\begin{columns}
	
		\column{0.8\textwidth}
		\centering
		\includegraphics[width=\linewidth]{with_poly.png}
		\textbf{With Poly}
	\end{columns}
	
	\vspace{1em}
    \textbf{Observation:} With polynomial regression (2nd degree) aligns correctly with the trend, improving model accuracy.
\end{frame}



	\begin{frame}{Spline Regression: Partial Residual Plot}
	\begin{columns}
		% Left Column: R Code
		\column{0.4\textwidth}
		\begin{tcolorbox}
			\begin{lstlisting}
# Fit spline model (Cubic B-Spline)
model_spline <- lm(price ~ bs(sqft_living, degree=3, df=6) + sqft_lot +
grade + bathrooms + bedrooms, data=house_df)

# Generate partial residual plot for spline regression
model_spline <- lm(price ~ bs(sqft_living, degree=3, df=6) + sqft_lot +
grade + bathrooms + bedrooms, data=house_df)
\end{lstlisting}
		\end{tcolorbox}
		
		% Right Column: Image
		\column{0.6\textwidth}
		\centering
		\includegraphics[width=\linewidth]{spline.png}
		\textbf{Spline Fit Visualization}
	\end{columns}
	
	\vspace{1em}
	\textbf{Explanation:} 
	\item	Cubic Splines: The function is made of piecewise cubic polynomials (degree = 3).
		\item Degrees of Freedom (df = 6): Controls flexibilityâ€”higher df allows more variation
			\item Splines fit the data smoothly without unnecessary fluctuations.
	\end{frame}
	
	\begin{frame}[fragile]{Nonlinear Fit: Splines in Python}
		\lstset{language=Python}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Practice}: Fits nonlinear price trends in zip 98105
				\item \textbf{Key Insight}: Better matches small vs. large home value shifts
			\end{itemize}
			\begin{lstlisting}
# Python (p. 190)
import statsmodels.formula.api as smf
formula = 'AdjSalePrice ~ bs(SqFtTotLiving, df=6, degree=3) + SqFtLot + Bathrooms + Bedrooms + BldgGrade'
model_spline = smf.ols(formula=formula, data=house_98105).fit()
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{itemize}
				\item Curves reflect reality
			\end{itemize}
		\end{columns}
	\end{frame}

	
	

	
	% Section 3: Refining Precision
	\section{Refining Precision}
	
	

	
	\begin{frame}{Model Assessment: Theory}
		\begin{itemize}
			\item \textbf{Theory (p. 153)}: Measures prediction quality and fit
			\item \textbf{RMSE}: $\sqrt{\frac{\sum (y_i - \hat{y}_i)^2}{n}}$â€”average error magnitude
			\item \textbf{$R^2$}: Proportion of variance explained (0-1); higher means better fit
			\item \textbf{Use}: Guides housing prediction accuracy
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Cross-Validation: Theory}
		\begin{itemize}
			\item \textbf{Theory (p. 155)}: Validates model on unseen data via $k$-fold splits
			\item \textbf{Process}: Divide data, train on $k-1$, test on 1, repeat, average RMSE
			\item \textbf{Why}: Ensures predictions generalize beyond training salesâ€”crucial for real estate
		\end{itemize}
	\end{frame}

	\begin{frame}{Picking the Best Story: Model Selection}
		\begin{itemize}
			\item Too many clues clutter the tale. Stepwise selection trims variables, AIC balances fit and simplicity, and penalties shrink extras.
			\item Think of it as editing: keep the essentials, cut the fluffâ€”Occamâ€™s razor guides us to a lean, powerful narrative.
			\item Our goal? A story thatâ€™s clear and predicts well, not a sprawling epic.
		\end{itemize}
	\end{frame}	
	
	\begin{frame}[fragile]{Model Selection: Theory \& R}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Theory (p. 156)}: Balances fit vs. complexityâ€”Occamâ€™s razor
				\item \textbf{AIC}: $2P + n\log(\text{RSS}/n)$â€”penalizes extra predictors
				\item \textbf{Goal}: Optimal housing model without overkill
			\end{itemize}
			\begin{lstlisting}
# R (p. 157)
library(MASS)
house_full <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +
Bedrooms + BldgGrade + PropertyType, data = house)
step <- stepAIC(house_full, direction = "both")
# Drops less impactful vars
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{itemize}
				\item Streamlined predictors
			\end{itemize}
		\end{columns}
	\end{frame}

	\begin{frame}[fragile]{Multiple Linear Regression: Key Findings}
	\begin{columns}
		\column{0.9\textwidth}
		\begin{lstlisting}
house_full <- lm(price ~ sqft_living + sqft_lot15 + bathrooms +
bedrooms + grade+yr_renovated , data = house_df)
step <- stepAIC(house_full, direction = "both")
summary(step)

Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -4.841e+05  1.477e+04 -32.777   <2e-16 ***
sqft_living   2.300e+02  3.597e+00  63.956   <2e-16 ***
sqft_lot15   -7.053e-01  6.252e-02 -11.282   <2e-16 ***
bathrooms    -3.096e+04  3.442e+03  -8.994   <2e-16 ***
bedrooms     -4.026e+04  2.271e+03 -17.726   <2e-16 ***
grade         9.776e+04  2.290e+03  42.693   <2e-16 ***
yr_renovated  8.746e+01  4.160e+00  21.022   <2e-16 ***

		\end{lstlisting}
		\begin{itemize}
      		\item sqft lot15 (-5.135e-01) Small , impact on price can be removal.
			\item grade (1.315e+05): Highest positive impact, should be kept.
	\end{itemize}
	\end{columns}
	\end{frame}
	
	
	\begin{frame}[fragile]{Model Selection: Python}
		\lstset{language=Python}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Practice}: Automates predictor choice for housing
				\item \textbf{Key Insight}: Reduces noise, enhances prediction
			\end{itemize}
			\begin{lstlisting}
# Python (p. 158 adapted)
from dmba import stepwise_selection
predictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms', 'BldgGrade']
def train_model(vars):
model = LinearRegression()
model.fit(house[vars], house[outcome])
return model
best_model, _ = stepwise_selection(house[predictors].columns, train_model)
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{itemize}
				\item Focused fit
			\end{itemize}
		\end{columns}
	\end{frame}
	
	\begin{frame}[fragile]{Weighted Regression: Theory \& R}
		\begin{columns}
			\column{0.5\textwidth}
			\begin{itemize}
				\item \textbf{Theory (p. 159)}: Weights adjust influence by reliability
				\item \textbf{Why}: Older sales less relevantâ€”recent data gets priority
				\item \textbf{Impact}: Refines coefficients for current market
			\end{itemize}
			\begin{lstlisting}
house_df$Weight <- house_df$Year - 2005
house_lm <- lm(price ~ sqft_living + sqft_lot + bathrooms +bedrooms + grade, data=house_df)# Fit unweighted linear regression
house_wt <- lm(price ~ sqft_living + sqft_lot + bathrooms +bedrooms + grade, data=house_df, weight=Weight)# Fit weighted linear regression
# Compare coefficients of both models
round(cbind(house_lm=house_lm$coefficients,house_wt=house_wt$coefficients), digits=3)
			\end{lstlisting}
			\column{0.5\textwidth}
			\begin{lstlisting}
               house_lm    house_wt
(Intercept) -471575.692 -472501.632
sqft_living     231.350     230.816
sqft_lot         -0.325      -0.317
bathrooms    -27973.439  -28075.162
bedrooms     -40744.142  -40639.355
grade         95586.697   95883.234
\end{lstlisting}
	\begin{itemize}
	\item here no significant as data set only contain 2014,2015 data
\end{itemize}
		\end{columns}
	\end{frame}
	
	\begin{frame}[fragile]{Weighted Regression: Python}
		\lstset{language=Python}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Practice}: Weights tune housing model
				\item \textbf{Key Insight}: Aligns predictions with market trends
			\end{itemize}
			\begin{lstlisting}
# Python (p. 160)
house['Weight'] = [int(date.split('-')[0]) for date in house.DocumentDate] - 2005
house_wt = LinearRegression()
house_wt.fit(house[predictors], house[outcome], sample_weight=house.Weight)
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{itemize}
				\item Fresher focus
			\end{itemize}
		\end{columns}
	\end{frame}
	
	% Section 4: Facing Challenges
	\section{Facing Challenges}
	
	\begin{frame}{Prediction Limits: Theory}
		\begin{itemize}
			\item \textbf{Theory (p. 161)}: Extrapolation beyond data failsâ€”e.g., empty lots
			\item \textbf{Intervals}: Confidence for $b_i$, wider prediction for $\hat{Y}_i$
			\item \textbf{Why}: Uncertainty spikes outside training rangeâ€”limits housing forecasts
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Interpreting Coefficients: Theory}
		\begin{itemize}
			\item \textbf{Theory (p. 171)}: Coefficients mislead if predictors correlate
			\item \textbf{Multicollinearity}: Size and bedrooms overlapâ€”unstable fits
			\item \textbf{Confounding}: Missing location skews results
			\item \textbf{Interactions}: Sizeâ€™s effect varies by zipâ€”needs modeling
		\end{itemize}
	\end{frame}
	
	\begin{frame}[fragile]{Diagnostics: Theory \& R}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Theory (p. 176)}: Residuals reveal model flaws
				\item \textbf{Outliers}: Extreme sales (e.g., \$119,748); \textbf{Influence}: Sway points
				\item \textbf{Heteroskedasticity}: Uneven errors signal gaps
			\end{itemize}
			\begin{lstlisting}
# R (p. 177)
house_98105 <- house[house$ZipCode == 98105, ]
lm_98105 <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +
Bedrooms + BldgGrade, data = house_98105)
sresid <- rstandard(lm_98105)  # -4.326732 outlier
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{figure}
				\includegraphics[width=\textwidth]{figure4-6-placeholder.pdf}
				\caption{Influence Plot}
			\end{figure}
		\end{columns}
	\end{frame}
	
	\begin{frame}[fragile]{Diagnostics: Python}
		\lstset{language=Python}
		\begin{columns}
			\column{0.6\textwidth}
			\begin{itemize}
				\item \textbf{Practice}: Identifies \$119,748 as partial sale anomaly
				\item \textbf{Key Insight}: Diagnostics ensure robust housing predictions
			\end{itemize}
			\begin{lstlisting}
# Python (p. 178)
from statsmodels.stats.outliers_influence import OLSInfluence
house_98105 = house[house['ZipCode'] == 98105]
model = smf.ols('AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade', data=house_98105).fit()
influence = OLSInfluence(model)
sresiduals = influence.resid_studentized
			\end{lstlisting}
			\column{0.4\textwidth}
			\begin{itemize}
				\item Spots critical flaws
			\end{itemize}
		\end{columns}
	\end{frame}
	
	\begin{frame}[fragile]{Influence Plot (Bubble Plot) â€“ Identify Influential Values}
		\begin{itemize}
			\item \textbf{Purpose:} Identifies influential observations by combining leverage, residuals, and Cookâ€™s Distance.
			\item \textbf{Key Insights:}
			\begin{itemize}
				\item Large bubbles = high Cookâ€™s Distance â†’ Removing these changes regression results significantly.
				\item Possible reasons:
				\begin{enumerate}
					\item High leverage (extreme predictor values) , Large residual (far from regression line) ,Both high leverage and large residual.
				\end{enumerate}
			\end{itemize}
			\item \textbf{Results:} Four large influential points found (Cookâ€™s D > 0.08), impacting coefficients.
			\item Residuals beyond Â±2.5
		\end{itemize}
		

	\end{frame}
	
	
	\begin{frame}[fragile]{Influence Plot (Bubble Plot) â€“ Identify Influential Values}
		\begin{columns}
		
			\column{0.8\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{influence_plot.png}
				\caption{Bubble plot showing influential points.}
			\end{figure}
		\end{columns}
	\end{frame}
	
	\begin{frame}[fragile]{Influence Plot (Bubble Plot) â€“ Identify Influential Values}
		\begin{itemize}
		\end{itemize}

				\begin{lstlisting}[language=R, caption=Influence Plot in R]
library(car)
lm_model <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade, data=house_98105)
influencePlot(lm_model)
		\end{lstlisting}
		\begin{lstlisting}[language=Python, caption=Influence Plot in Python]
house_98105 = house[house['ZipCode'] == 98105]
X = house_98105[['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms', 'BldgGrade']].assign(const=1)
y = house_98105['AdjSalePrice']
model = sm.OLS(y, X).fit()
influence = sm.stats.outliers_influence.OLSInfluence(model)
fig, ax = plt.subplots(figsize=(5, 5))
ax.axhline(-2.5, ls='--', color='C1')
ax.axhline(2.5, ls='--', color='C1')
ax.scatter(influence.hat_matrix_diag, influence.resid_studentized_internal,
s=1000 * np.sqrt(influence.cooks_distance[0]), alpha=0.5)
ax.set_xlabel('hat values')
ax.set_ylabel('studentized residuals')
plt.show()\
		\end{lstlisting}
	\end{frame}
	
		\begin{frame}{Residual Plot (Heteroskedasticity Check)}
		\begin{itemize}
			\item \textbf{Purpose:} The residual plot checks for heteroskedasticity by analyzing how residuals (errors) vary with predicted values.
			\item \textbf{Key Insights:}
			\begin{itemize}
				\item X-axis (Predicted Values): Represents the fitted values from the regression model.
				\item Y-axis (Absolute Residuals): Measures the deviation of actual values from predictions.
				\item Scatter Points: Each dot represents an observationâ€™s residual.
				\item LOESS Smoother (Blue Line): Shows the trend in residuals.
				\item Shaded Region: Indicates confidence around the trend.
				\begin{enumerate}
					\item Heteroskedasticity detected â€“ Residuals increase with larger predicted values, indicating variance instability.
					\item  Curved Trend â€“ Suggests missing variables or non-linearity in the data.
					\item Outliers at High Predictions â€“ Some extreme points have large residuals, further confirming instability.
				\end{enumerate}
			\end{itemize}
		\end{itemize}
	\end{frame}
	
	
		
	\begin{frame}{Residual Plot (Heteroskedasticity Check)}
		\begin{columns}
			
			\column{0.7\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{residual_plot.png}
				\caption{Bubble plot showing influential points.}
			\end{figure}
		\end{columns}
	\end{frame}
	
	\begin{frame}[fragile]{Influence Plot (Bubble Plot) â€“ Identify Influential Values}
		\begin{itemize}
		\end{itemize}
		
		\begin{lstlisting}[language=R, caption=Heteroskedasticity Plot in R]
df <- data.frame(resid = residuals(lm_98105), pred = predict(lm_98105))
ggplot(df, aes(pred, abs(resid))) + geom_point() + geom_smooth()
		\end{lstlisting}
		\begin{lstlisting}[language=Python, caption=Heteroskedasticity Plot in Python]
import seaborn as sns
fig, ax = plt.subplots(figsize=(5, 5))
sns.regplot(model.fittedvalues, np.abs(model.resid), scatter_kws={'alpha': 0.25},
line_kws={'color': 'C1'}, lowess=True, ax=ax)
ax.set_xlabel('predicted')
ax.set_ylabel('abs(residual)')
plt.show()
		\end{lstlisting}
	\end{frame}
	
	\begin{frame}{Histogram of Standardized Residuals: Normality Check}
		\begin{itemize}
			\item \textbf{Purpose:} The histogram assesses residual normality by analyzing the distribution of standardized residuals.
			\item \textbf{Key Insights:}
			\begin{itemize}
				\item Centering Around Zero: The residuals are centered around 0, indicating no strong systematic bias in the model.
				\item Skewness & Long Tails: The right tail is longer, suggesting right-skewness and possible underestimation of some values.
				
				\item Non-Normal Distribution: The residuals deviate from a perfect bell-shaped curve, hinting at potential issues:
					\begin{enumerate}
						\item 	âœ… Missing predictors affecting the model.
						\item 	âœ… Heteroskedasticity, as observed in the residual plot.
						\item 	âœ… Outliers influencing the regression fit.
					\end{enumerate}
			\end{itemize}
		\end{itemize}
	\end{frame}



	\begin{frame}{Histogram of Standardized Residuals: Normality Check}
		\begin{columns}
			
			\column{0.8\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{histo.png}
				\caption{Bubble plot showing influential points.}
			\end{figure}
		\end{columns}
	\end{frame}

\begin{frame}[fragile]{Histogram of Standardized Residuals: Normality Check}
	\begin{itemize}
	\end{itemize}
	
	\begin{lstlisting}[language=R, caption=Histogram of Standardized Residuals in R]
df <- data.frame(resid = residuals(lm_98105), pred = predict(lm_98105))
ggplot(df, aes(pred, abs(resid))) + geom_point() + geom_smooth()
	\end{lstlisting}
	\begin{lstlisting}[language=Python, caption=Histogram of Standardized Residuals in Python]
plt.hist(influence.resid_studentized_internal, bins=50, color='lightblue')
plt.xlabel('Standardized Residuals')
plt.title('Histogram of Standardized Residuals')
plt.show()
	\end{lstlisting}
\end{frame}
	
	% Section 5: Insights & Next Steps
	\section{Insights \& Next Steps}
	
	\begin{frame}{Housing Insights}
		\begin{itemize}
			\item \textbf{Findings}: Linear ties price to size, splines capture nonlinear trends
			\item \textbf{Diagnostics}: Reveal quirks like partial salesâ€”critical for accuracy
			\item \textbf{Application}: Real-world tool for buyers, sellers, and assessors
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Key Takeaways}
		\begin{itemize}
			\item \textbf{Flexible}: Evolves from simple lines to complex curves for housing
			\item \textbf{Precise}: RMSE and cross-validation ensure reliable price predictions
			\item \textbf{Powerful}: R and Python implementations unlock data-driven insights
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Looking Ahead}
		\begin{itemize}
			\item \textbf{Resources}: \textit{Statistical Learning} (Hastie et al.), \textit{Time Series Forecasting} (Shmueli)
			\item \textbf{Next Steps}: Dive into splines, time series for dynamic housing models
			\item \textbf{Call}: Blend theory and practice for smarter predictions
		\end{itemize}
	\end{frame}
	
\end{document}